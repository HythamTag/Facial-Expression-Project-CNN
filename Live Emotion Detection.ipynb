{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2 \nimport numpy as np\nfrom keras.models import load_model\nimport time\nfrom mtcnn.mtcnn import MTCNN\n\n\ndef Detect_Faces(image, model):\n    return face_detection_model.detect_faces(image)\n\ndef Draw_Bounding_BOX(Face_points, image, color_box):\n    overlay_face = image.copy()\n    overlay_top = image.copy()\n    alpha_face=0.8\n    alpha_tope=0.5\n    shift =1\n    x_point, y_point, width, height = Face_points\n    \n    # print(Face_points)\n    cv2.rectangle(overlay_face,\n              (Face_points[0], Face_points[1]),\n              (Face_points[0]+Face_points[2], Face_points[1] + Face_points[3]),\n              color_box,\n              2)\n        \n\n    #cv2.rectangle(overlay_top, (x_point - shift // 2, y_point - height//20), (x_point + width + shift // 2, y_point - height//5), (255,128,0), -1)\n    cv2.rectangle(overlay_top, (x_point - shift, y_point - shift ), (x_point + width + shift, y_point - height//5 - shift), (255,128,0), -1)\n    \n    cv2.addWeighted(overlay_face, alpha_face, image, 1 - alpha_face, 0, image)\n    cv2.addWeighted(overlay_top, alpha_tope, image, 1 - alpha_tope, 0, image)\n    \ndef put_emoji(image,Face_points,Emoji_images,emotion_num):\n\n    x_point, y_point, width, height = Face_points\n    shift = 1\n    height_rec = 0.2*height\n    offset = int(height_rec*0.05)\n    \n    emotion_size = int(height_rec - offset*2)\n    \n    x2 = int(x_point - offset + width + shift)\n    x1 = int(x2 - emotion_size )\n    \n    y1 = int(y_point - offset - shift)\n    y2 = int(y1 - emotion_size)\n\n    emoji_img = cv2.resize(Emoji_images[emotion_num],(int(emotion_size),int(emotion_size)))\n    \n    mask =np.array((emoji_img==255), np.uint8)\n\n    image[y2:y1,x1:x2] =  image[y2:y1,x1:x2]*mask + emoji_img*(1-mask)  # dont forget in lost [smaller : larger] no the reverse\n    return image\n    \n    \n    \ndef ROI_Face_Frame(image,Face_points):\n    x_point, y_point, width, height = Face_points\n    x_offset=int(width/20.)\n    y_offset=int(height/20.)\n    x1=x_point + x_offset\n    x2=x_point + width - x_offset\n    y1=y_point - y_offset\n    y2=y_point + height \n    face = image[y1:y2,x1:x2]\n    return face\n\n#load models \nmodel1 = 'weights.51-1.19.hdf5'\nmodel2 ='weights.37-0.91.hdf5'\nmodel3 ='model_weights.h5'\nmodel4 ='weights.08-1.19.hdf5'\nmodel5 = 'mini_XCEPTION_KDEF.hdf5'\nmodel6 = 'tiny_XCEPTION_KDEF.hdf5'\nmodel7 = 'simple_CNN.985-0.66.hdf5'\nmodel8 = 'simple_CNN.530-0.65.hdf5'\nmodel9 = 'fer2013_mini_XCEPTION.102-0.66.hdf5'\nmodel10 = 'weights.51-0.313-conv-32-nodes-0-dense-1568549063.hdf5'\n\nmodel11 = 'weights.37-0.29-4-conv-256-nodes-0-dense-1568581634.hdf5'\nmodel12 = 'weights.46-0.28-4-conv-128-nodes-1-dense-1568590843.hdf5'\nmodel13 = 'weights.13-0.29-5-conv-64-nodes-0-dense-1568685072-Trial2.hdf5'\nmodel_sep = 'weights.73-0.83last.hdf5'\nweight_last = 'weights.41-0.24-3-conv-64-nodes-2-dense-1569112505.hdf5'\nvvlast = 'weights.30-0.23-4-conv-128-nodes-2-dense-1569285696.hdf5'\nemotion_classifier = load_model(vvlast, compile=False)\n\n\n","metadata":{},"execution_count":1,"outputs":[{"name":"stderr","output_type":"stream","text":"Using TensorFlow backend.\n"},{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"}]},{"cell_type":"code","source":"\nface_detection_model = MTCNN()\n\ncv2.namedWindow('Live')\nvideo_capture = cv2.VideoCapture(0)\n#EMOTIONS_LIST = ['angry', 'happy', 'sad', 'surprise', 'neutral']\nEMOTIONS_LIST = ['angry', 'happy', 'sad', 'surprise', 'neutral']\nEmoji_images = [cv2.imread('Emojis\\{}.png'.format(no)) for no in range(1,6)]\nsize = (int(video_capture.read()[1].shape[1]*1.8),int(video_capture.read()[1].shape[0]*1.8))\n\n# thanks = cv2.resize(cv2.imread('thanks.png') , size )\n# cv2.imshow('Live',thanks)\n# cv2.waitKey(3000)\n\n\nwhile True:\n    bgr_frame = video_capture.read()[1]\n\n    gray_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2GRAY)\n\n    face_points = Detect_Faces(bgr_frame,face_detection_model)\n    \n    \n    if face_points != []:\n        for person in face_points:\n            box = person['box']\n            if box[2]*box[3] > 10000:\n               # print(one_face_points[2]*one_face_points[3] )\n                Draw_Bounding_BOX(box,bgr_frame,[0,0,0])\n                \n                ROI_Face=ROI_Face_Frame(gray_frame,box)\n                #cv2.imshow('face', ROI_Face)\n                ROI_Face_Predict = cv2.resize(ROI_Face.copy(),(48,48))/255.\n                \n                ROI_Face_Predict=ROI_Face_Predict[np.newaxis, :, :, np.newaxis]\n                \n               # print(ROI_Face_Predict.shape)\n                custom = (emotion_classifier.predict(ROI_Face_Predict)*100.).tolist()[0]\n                \n#                 print(custom)\n                custom.pop(1)\n                custom.pop(1)\n#                 print(custom)\n                emotion_num = np.argmax(custom)\n                \n                put_emoji(bgr_frame, box, Emoji_images, emotion_num)\n                \n                emotion = EMOTIONS_LIST[emotion_num] + \" \" + str(int(np.max(custom))) + \" %\"\n                \n                cv2.putText(bgr_frame, emotion, (10,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n                \n       # print(custom)\n    \n    bgr_frame=cv2.resize(bgr_frame,(int(bgr_frame.shape[1]*1.8),int(bgr_frame.shape[0]*1.8)))\n    cv2.imshow('Live', bgr_frame)\n    #cv2.imshow('grey', gray_frame)\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\nvideo_capture.release()\ncv2.destroyAllWindows()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}