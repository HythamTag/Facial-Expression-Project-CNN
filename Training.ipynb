{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "import pandas as pd \n",
    "#from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "width = 48\n",
    "height = 48\n",
    "num_classes = 7\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "patience = 50\n",
    "\n",
    "input_shape = (48, 48, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpu - gpu configuration\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) #max: 1 gpu, 56 cpu\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "Emotion_Data_Set_Path = \"Data/Face_Expression_Detection_Model/fer2013.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_fer2013 = pd.read_csv(Emotion_Data_Set_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DF_fer2013[DF_fer2013.Usage == 'Training'] .reset_index()\n",
    "test_set  = DF_fer2013[DF_fer2013.Usage == 'PublicTest'].reset_index()\n",
    "\n",
    "\n",
    "train_data_list=train_set.pixels.tolist()\n",
    "test_data_list =test_set.pixels.tolist()\n",
    "train_label= train_set.emotion.tolist()\n",
    "test_label = test_set.emotion.tolist()\n",
    "\n",
    "train_label = to_categorical(train_set.emotion)\n",
    "test_label = to_categorical(test_set.emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(set):\n",
    "    list= []\n",
    "    for line in set:\n",
    "        list.append(line.split())\n",
    "    return list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_string(train_data_list)\n",
    "test_data= process_string(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_data,'float32')\n",
    "y_train = np.array(train_label,'float32')\n",
    "x_test  = np.array(test_data,'float32')\n",
    "y_test  = np.array(test_label,'float32')\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], width, height, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], width, height,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= x_train/255.0\n",
    "x_test= x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train Shape: (28709, 48, 48, 1)\n",
      "Y_Train Shape: (28709, 7)\n",
      "X_Test Shape: (3589, 48, 48, 1)\n",
      "X_Test Shape: (3589, 7)\n",
      "X_Train dtype: float32\n",
      "Y_Train dtype: float32\n",
      "X_Test dtype: float32\n",
      "X_Test dtype: float32\n",
      "X_Train ndim: 4\n",
      "Y_Train ndim: 2\n",
      "X_Test ndim: 4\n",
      "X_Test ndim: 2\n",
      "X_Train size: 66145536\n",
      "Y_Train size: 200963\n",
      "X_Test size: 8269056\n",
      "X_Test size: 25123\n",
      "X_Train dtype.name: float32\n",
      "Y_Train dtype.name: float32\n",
      "X_Test dtype.name: float32\n",
      "X_Test dtype.name: float32\n",
      "number of images:  32298\n",
      "instance length train:  (28709, 48, 48, 1)\n",
      "instance length test:  (3589, 48, 48, 1)\n",
      "28709 train samples\n",
      "3589 test samples\n"
     ]
    }
   ],
   "source": [
    "#------------------------------#------------------------------\n",
    "print(\"X_Train Shape: {}\".format(x_train.shape))\n",
    "print(\"Y_Train Shape: {}\".format(y_train.shape))\n",
    "print(\"X_Test Shape: {}\".format(x_test.shape))\n",
    "print(\"X_Test Shape: {}\".format(y_test.shape))\n",
    "\n",
    "print(\"X_Train dtype: {}\".format(x_train.dtype))\n",
    "print(\"Y_Train dtype: {}\".format(y_train.dtype))\n",
    "print(\"X_Test dtype: {}\".format(x_test.dtype))\n",
    "print(\"X_Test dtype: {}\".format(y_test.dtype))\n",
    "\n",
    "print(\"X_Train ndim: {}\".format(x_train.ndim))\n",
    "print(\"Y_Train ndim: {}\".format(y_train.ndim))\n",
    "print(\"X_Test ndim: {}\".format(x_test.ndim))\n",
    "print(\"X_Test ndim: {}\".format(y_test.ndim))\n",
    "\n",
    "print(\"X_Train size: {}\".format(x_train.size))\n",
    "print(\"Y_Train size: {}\".format(y_train.size))\n",
    "print(\"X_Test size: {}\".format(x_test.size))\n",
    "print(\"X_Test size: {}\".format(y_test.size))\n",
    "\n",
    "print(\"X_Train dtype.name: {}\".format(x_train.dtype.name))\n",
    "print(\"Y_Train dtype.name: {}\".format(y_train.dtype.name))\n",
    "print(\"X_Test dtype.name: {}\".format(x_test.dtype.name))\n",
    "print(\"X_Test dtype.name: {}\".format(y_test.dtype.name))\n",
    "\n",
    "print(\"number of images: \",len(x_test)+len(x_train))\n",
    "print(\"instance length train: \",x_train.shape)\n",
    "print(\"instance length test: \",x_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "#------------------------------#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Model_v1(input_shape,num_classes):\n",
    "    #construct CNN structure\n",
    "    model = Sequential()\n",
    "\n",
    "    #1st convolution layer\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "\n",
    "    #2nd convolution layer\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "    #3rd convolution layer\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #fully connected neural networks\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(rate = 1 - 0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(rate = 1 - 0.2))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint(\n",
    "    filepath='Trained Models/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    mode='max')\n",
    "rlrop = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience / 4), verbose=1)\n",
    "\n",
    "csvl = CSVLogger(\n",
    "    filename='tmp/training.log',\n",
    "    separator=',',\n",
    "    append=False)\n",
    "ts = TensorBoard(log_dir='tmp')\n",
    "\n",
    "callbacks = [mc, es, rlrop, csvl, ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "256/256 [==============================] - 12s 48ms/step - loss: 0.0072 - acc: 0.2460 - val_loss: 2.6592 - val_acc: 0.2494\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.65924, saving model to Trained Models/weights.01-2.66.hdf5\n",
      "Epoch 2/1000\n",
      "256/256 [==============================] - 12s 46ms/step - loss: 0.0070 - acc: 0.2574 - val_loss: 13.0271 - val_acc: 0.1452\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.65924\n",
      "Epoch 3/1000\n",
      "256/256 [==============================] - 12s 47ms/step - loss: 0.0064 - acc: 0.3444 - val_loss: 13.2790 - val_acc: 0.1591\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.65924\n",
      "Epoch 4/1000\n",
      "137/256 [===============>..............] - ETA: 5s - loss: 0.0060 - acc: 0.4044"
     ]
    }
   ],
   "source": [
    "\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#------------------------------\n",
    "model = CNN_Model_v1(input_shape,num_classes)\n",
    "model.compile(loss='categorical_crossentropy'\n",
    "    , optimizer=keras.optimizers.Adam()\n",
    "    , metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs, callbacks=callbacks, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = validation_generator,\n",
    "                                validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "                                callbacks=callbacks\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit_generator(train_generator, steps_per_epoch=len(x_train) / batch_size, epochs=epochs ,callbacks=callbacks , verbose=1, validation_data=val_data)\n",
    "#model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs,validation_data=val_data,callbacks=callbacks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
